import feedparser
import datetime
from datetime import timedelta, timezone
import re
import trafilatura
import os

# ---------------------------------------------------------
# 1. í•œêµ­ ì‹œê°„(KST) ì„¤ì •
# ---------------------------------------------------------
KST = timezone(timedelta(hours=9))

def get_korea_time():
    return datetime.datetime.now(KST)

# ---------------------------------------------------------
# 2. ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ (ê°€ë…ì„± ê°œì„ : ì¤„ë°”ê¿ˆ ì¶”ê°€)
# ---------------------------------------------------------
def get_clean_summary(url):
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded is None: return None

        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
        if not text or len(text) < 50: return None

        # í…ìŠ¤íŠ¸ ì •ì œ
        text = text.replace('\n', ' ').strip()
        text = re.sub(r'\s+', ' ', text)

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = text.split('. ')
        summary_sentences = []
        char_count = 0
        
        for sent in sentences:
            clean_sent = sent.strip()
            if len(clean_sent) < 20: continue
            if not clean_sent.endswith('.'): clean_sent += '.'
            
            # ê°€ë…ì„±ì„ ìœ„í•´ ë¬¸ì¥ ì•ì— ì¸ìš©êµ¬(>) ì¶”ê°€ ë° ì¤„ë°”ê¿ˆ ì²˜ë¦¬
            summary_sentences.append(f"> {clean_sent}")
            char_count += len(clean_sent)
            
            if char_count > 300: break # í•µì‹¬ 3~4ë¬¸ì¥ë§Œ
        
        # ì¤„ë°”ê¿ˆ(\n>)ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ê°€ë…ì„± í™•ë³´
        return '\n'.join(summary_sentences) if summary_sentences else None

    except Exception:
        return None

# ---------------------------------------------------------
# 3. ë©”ì¸ ë¡œì§
# ---------------------------------------------------------
def fetch_news():
    sources = {
        "ğŸ¤– ì¸ê³µì§€ëŠ¥ (AI)": "http://www.aitimes.com/rss/allArticle.xml",
        "ğŸ’° ê²½ì œ": "https://www.hankyung.com/feed/economy", 
        "ğŸ“ êµìœ¡": "http://www.veritas-a.com/rss/allArticle.xml" 
    }
    
    now = get_korea_time()
    today_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%I:%M:%S")
    
    # ì˜¤ì „/ì˜¤í›„ íƒœê·¸ ì„¤ì •
    time_tag = "ì˜¤ì „" if now.hour < 12 else "ì˜¤í›„"
    
    # ì „ì²´ ë‰´ìŠ¤ ì»¨í…ì¸ ë¥¼ ë‹´ì„ ë³€ìˆ˜
    news_content = ""
    # ì œëª©ì— ì‚¬ìš©í•  ëŒ€í‘œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    headlines = []

    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    for category, rss_url in sources.items():
        news_content += f"## {category}\n"
        
        try:
            feed = feedparser.parse(rss_url)
            # ì¹´í…Œê³ ë¦¬ë³„ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì œëª©ì„ í—¤ë“œë¼ì¸ í›„ë³´ë¡œ ì €ì¥
            if feed.entries:
                first_title = feed.entries[0].title
                # ë„ˆë¬´ ê¸´ ì œëª©ì€ 15ìë¡œ ìë¦„
                short_title = first_title[:15] + "..." if len(first_title) > 15 else first_title
                headlines.append(short_title)

            # ê¸°ì‚¬ 3ê°œì”© ì¶”ì¶œ
            for entry in feed.entries[:3]:
                summary = get_clean_summary(entry.link)
                
                # ë³¸ë¬¸ ìš”ì•½ ì‹¤íŒ¨ ì‹œ RSS ì„¤ëª… ì‚¬ìš©
                if not summary:
                    desc = entry.get('description', 'ìš”ì•½ ì—†ìŒ')
                    desc = re.sub(r'<[^>]+>', '', desc)[:100] + "..."
                    summary = f"> {desc}"
                
                news_content += f"### ğŸ”— [{entry.title}]({entry.link})\n"
                news_content += f"{summary}\n\n" # ìš”ì•½ë¬¸ (ì¤„ë°”ê¿ˆ ì ìš©ë¨)
                
        except Exception as e:
            news_content += f"> âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—ëŸ¬: {e}\n\n"

    # ---------------------------------------------------------
    # 4. íŒŒì¼ ì œëª© ë° Frontmatter ìƒì„±
    # ---------------------------------------------------------
    
    # í—¤ë“œë¼ì¸ ìƒì„± (ì˜ˆ: AI í˜ëª…... / ê¸ˆë¦¬ ì¸ìƒ... / ìˆ˜ëŠ¥ ê°œí¸...)
    headline_str = " / ".join(headlines) if headlines else "ì£¼ìš” ë‰´ìŠ¤ ë¸Œë¦¬í•‘"
    
    # Frontmatter ì‘ì„± (ì˜µì‹œë””ì–¸ìš© ë©”íƒ€ë°ì´í„°)
    frontmatter = f"""---
date: {today_str}
time: {time_str}
type: insight
tags: [ë‰´ìŠ¤, {time_tag}, ìë™í™”]
created_at: {today_str} {time_str}
---

# ğŸ“… {today_str} {time_tag} ë¸Œë¦¬í•‘: {headline_str}

"""
    
    # ìµœì¢… ë³¸ë¬¸ ê²°í•©
    final_content = frontmatter + news_content
    final_content += "---\n"
    final_content += f"âœ… **ìµœì¢… ì—…ë°ì´íŠ¸(í•œêµ­ì‹œê°„):** {today_str} {time_str}\n"

    # íŒŒì¼ëª… ìƒì„± (ì˜ˆ: 2026-01-27_ì˜¤ì „_Daily_News_Briefing.md)
    filename = f"{today_str}_{time_tag}_{time_str}_Daily_News_Briefing.md"
    
    return filename, final_content

if __name__ == "__main__":
    filename, content = fetch_news()
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"File Created: {filename}")
