import feedparser
import datetime
import re
import trafilatura

def get_clean_summary(url):
    try:
        # User-Agentë¥¼ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥í•˜ì—¬ ì°¨ë‹¨ ë°©ì§€
        downloaded = trafilatura.fetch_url(url)
        
        if downloaded is None:
            return ""

        # ë³¸ë¬¸ ì¶”ì¶œ
        text = trafilatura.extract(downloaded, include_comments=False)
        
        if not text or len(text) < 50:
            return ""

        # í…ìŠ¤íŠ¸ ì •ì œ
        text = text.replace('\n', ' ').strip()
        text = re.sub(r'\s+', ' ', text)

        # ë¬¸ì¥ ë¶„ë¦¬ ë° ìš”ì•½
        sentences = text.split('. ')
        summary_sentences = []
        char_count = 0
        
        for sent in sentences:
            clean_sent = sent.strip()
            if len(clean_sent) < 15: continue
            
            if not clean_sent.endswith('.'):
                clean_sent += '.'
            
            summary_sentences.append(clean_sent)
            char_count += len(clean_sent)
            
            if char_count > 300: # ìš”ì•½ ê¸¸ì´ ìµœì í™”
                break
        
        return ' '.join(summary_sentences) if summary_sentences else ""

    except Exception as e:
        print(f"Error: {url} -> {e}")
        return ""

def fetch_news():
    # êµìœ¡ ì†ŒìŠ¤ë¥¼ ë” ì•ˆì •ì ì¸ 'ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
    sources = {
        "ğŸ¤– ì¸ê³µì§€ëŠ¥ (AI)": "http://www.aitimes.com/rss/allArticle.xml",
        "ğŸ’° ê²½ì œ": "https://www.hankyung.com/feed/economy", 
        "ğŸ“ êµìœ¡": "http://www.veritas-a.com/rss/allArticle.xml" 
    }
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    
    markdown = f"""---
date: {today_str}
last_update: {now.strftime("%Y-%m-%d %H:%M:%S")}
type: insight
topic: [ì¸ê³µì§€ëŠ¥, ê²½ì œ, êµìœ¡]
tags: [ë‰´ìŠ¤, ìš”ì•½, {today_str}]
source: [AIíƒ€ì„ìŠ¤, í•œêµ­ê²½ì œ, ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ]
---

# ğŸ“… {now.strftime('%Yë…„ %mì›” %dì¼(%a)')} í•µì‹¬ ë‰´ìŠ¤ ë¸Œë¦¬í•‘

"""
    
    for category, rss_url in sources.items():
        markdown += f"## {category}\n"
        try:
            feed = feedparser.parse(rss_url)
            success_count = 0
            
            for entry in feed.entries:
                if success_count >= 2: break
                
                summary = get_clean_summary(entry.link)
                
                if not summary:
                    continue
                
                markdown += f"### ğŸ”— [{entry.title}]({entry.link})\n"
                markdown += f"> {summary}\n\n"
                success_count += 1
                
        except Exception:
            markdown += "> í•´ë‹¹ ë¶„ì•¼ì˜ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"

    markdown += "---\n### ğŸ“‚ ìë™í™” ê¸°ë¡ ì•ˆë‚´\n"
    markdown += f"ìµœì¢… ì—…ë°ì´íŠ¸ ì‹œê°: **{now.strftime('%Y-%m-%d %H:%M:%S')}**\n"
    
    # íŒŒì¼ëª…ì„ ì˜ì–´ë¡œ ê³ ì •í•˜ì—¬ ì¸ì½”ë”© ê¹¨ì§ ë°©ì§€
    filename = f"{today_str}_Daily_News_Briefing.md"
    return filename, markdown

if __name__ == "__main__":
    filename, content = fetch_news()
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"Created: {filename}")
