import feedparser
import datetime
import re
from newspaper import Article

def get_article_content(url):
    """
    ê¸°ì‚¬ URLì„ íƒ€ê³  ë“¤ì–´ê°€ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ê³ , 
    ì•ë¶€ë¶„(í•µì‹¬ ë¦¬ë“œë¬¸)ì„ ì•½ 300~400ì ì •ë„ë¡œ ì˜ë¼ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        
        # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê³µë€)
        text = article.text.strip()
        
        if len(text) < 50: # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
            return ""

        # ê°€ë…ì„±ì„ ìœ„í•´ ë¬¸ë‹¨ ì •ë¦¬ (ì¤„ë°”ê¿ˆ ê³¼ë‹¤ ì œê±°)
        text = re.sub(r'\n+', ' ', text)
        
        # í•µì‹¬ ë‚´ìš©ì¸ ì•ë¶€ë¶„ 350ì ì¶”ì¶œ (ë¬¸ì¥ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šê²Œ ë§ˆì¹¨í‘œ ì²˜ë¦¬)
        summary = text[:350]
        if "." in summary[300:]: # 300ì ì´í›„ ì²« ë§ˆì¹¨í‘œì—ì„œ ëŠê¸°
            summary = summary[:300] + summary[300:].split('.')[0] + "."
        else:
            summary += "..."
            
        return summary
    except Exception as e:
        return ""

def fetch_korean_news():
    # 1. êµ­ë‚´ ê¶Œìœ„ ìˆëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ (ë²ˆì—­ ë¶ˆí•„ìš”)
    sources = {
        "ğŸ¤– ì¸ê³µì§€ëŠ¥ (AI)": "http://www.aitimes.com/rss/allArticle.xml", # êµ­ë‚´ AI ì „ë¬¸ì§€ 1ìœ„
        "ğŸ›ï¸ ì •ì¹˜": "https://www.yna.co.kr/rss/politics.xml", # ì—°í•©ë‰´ìŠ¤ (íŒ©íŠ¸ ìœ„ì£¼)
        "ğŸ¥ ì‚¬íšŒ": "https://www.yna.co.kr/rss/society.xml", # ì—°í•©ë‰´ìŠ¤
        "ğŸ“ êµìœ¡": "http://www.hangyo.com/rss/allArticle.xml" # í•œêµ­êµìœ¡ì‹ ë¬¸ (êµì´)
    }
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    today_kr = now.strftime("%Yë…„ %mì›” %dì¼(%a)")
    
    # 2. ë§ˆí¬ë‹¤ìš´ í—¤ë” ì‘ì„±
    markdown = f"---\ndate: {today_str}\ntags: [ë‰´ìŠ¤, ìŠ¤í¬ë©, {today_str}]\n---\n\n"
    markdown += f"# ğŸ“… {today_kr} ë¶„ì•¼ë³„ í•µì‹¬ ë‰´ìŠ¤ ë¸Œë¦¬í•‘\n\n"
    markdown += f"êµ­ë‚´ ì£¼ìš” ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì •ë¦¬ëœ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤. ì œëª©ì„ í´ë¦­í•˜ë©´ ì›ë¬¸ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
    
    first_title = "" 

    for category, rss_url in sources.items():
        markdown += f"## {category}\n"
        
        try:
            feed = feedparser.parse(rss_url)
            # ë¶„ì•¼ë³„ ìµœì‹  ê¸°ì‚¬ 2~3ê°œ ì„ ì •
            count = 0
            for entry in feed.entries:
                if count >= 2: break # ë¶„ì•¼ë³„ 2ê°œë§Œ (ë„ˆë¬´ ê¸¸ì–´ì§ ë°©ì§€)
                
                # (1) ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                content_summary = get_article_content(entry.link)
                
                # ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìœ¼ë©´ RSS ê¸°ë³¸ ì„¤ëª… ì‚¬ìš©
                if not content_summary:
                    if 'description' in entry:
                        content_summary = re.sub('<[^<]+?>', '', entry.description)[:200] + "..."
                    else:
                        continue # ë‚´ìš©ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€

                # (2) ì¶œë ¥ í¬ë§·: [ì œëª©](ë§í¬) + ë‚´ìš©
                markdown += f"### ğŸ”— [{entry.title}]({entry.link})\n"
                markdown += f"> {content_summary}\n\n"

                # íŒŒì¼ëª… ìƒì„±ìš© (ì²« ê¸°ì‚¬ ì œëª©)
                if not first_title:
                    first_title = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', entry.title)[:15]
                
                count += 1

        except Exception as e:
            print(f"Error processing {category}: {e}")
            markdown += "ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"

    # 3. í‘¸í„° ì‘ì„±
    markdown += "---\n"
    markdown += "### ğŸ“‚ ìë™í™” ê¸°ë¡ ì•ˆë‚´\n"
    markdown += f"ìœ„ ë‚´ìš©ì€ GitHub Actionsë¥¼ í†µí•´ êµ­ë‚´ ì–¸ë¡ ì‚¬ RSSì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
    
    return f"{today_str}_{first_title}.md", markdown

if __name__ == "__main__":
    filename, content = fetch_korean_news()
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"File created: {filename}")
